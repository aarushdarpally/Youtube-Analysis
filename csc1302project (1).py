# -*- coding: utf-8 -*-
"""CSC1302Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gTlQ8NUwdtipJlGfxFhLvRxYWb6Z9pcR

1. Choose a raw data collection of your choice (must have at least 300 data points and 5 features, i.e., 300 rows and 5 columns). 

We chose Spotify and Youtube dataset:

https://www.kaggle.com/datasets/salvatorerastelli/spotify-and-youtube

2. (20 points) Data preprocessing.

a.	Read in the data.

Using files.upload() function to upload the Spotify_Youtube.csv file from local driver. You will need to download the csv file to your computer first. 

Then, using pd.read_csv() function to read the data.
"""

import pandas as pd
import matplotlib.pyplot as plt

from google.colab import files

uploaded = files.upload()

# Go to the 3rd part ('Calculate some statistics') and run any code from there, if you can run it, then the csv file is already uploaded, skip these things on the 2nd part ('Data preprocessing').
# If you can NOT run it, you may need to run this part again and then 'Choose Files' again from your computer. Please don't forget to download the csv file from the link first. Then, run it again. 
# It may take a few minutes to run all the data since this is a big data.

import io

df = pd.read_csv(io.BytesIO(uploaded['Spotify_Youtube.csv']))
print(df)

df.info()
df.describe()

"""
b.	Clean the data. You may need to fill in the missing data or delete the data points where there is missing data. Make sure that you still have at least 300 data points after data cleaning. 

First, remove unused columns. 

Second, fill missing values with the mean of the columns.

Then, remove duplicates."""

# Remove unused columns.
zdf = df.drop(['Unnamed: 0', 'Url_spotify', 'Uri', 'Url_youtube', 'Description'], axis=1)
# Fill missing values with the mean of the columns.
df = df.fillna(df.mean())
# Remove duplicates.
df= df.drop_duplicates()

print(df)

# This is the dataset after the cleaning data part.
df.info()
df.describe()

"""3. (20 points) Calculate some statistics.

a. Choose at least two features (i.e., columns) to calculate their mean, median, variance, standard deviation, etc.

I chose these columns (Danceability, Energy, and Key) to calculate their mean, median, variance, and standard deviation.

I used mean(), median(), var(), and std() functions.

b. Describe what you find given the statistics.

i. Calculate and print mean, median, variance, and standard deviation of Danceability column. 
Then, describe what you find.
"""

mean1 = df['Danceability'].mean()
median1 = df['Danceability'].median()
variance1 = df['Danceability'].var()
std_deviation1 = df['Danceability'].std()

print(f'Mean of Danceability column is: {mean1: .4f}')
print(f'Median of Danceability column is: {median1: .4f}')
print(f'Variance of Danceability column is: {variance1: .4f}')
print(f'Standard deviation of Danceability column is: {std_deviation1: .4f}')

"""Definition:

Danceability: describes how suitable a track is for dancing based on musical elements. A value of 0.0 is least danceable and 1.0 is most danceable.

Danceability column of the dataset says the following:
- The mean danceability score for the dataset is slightly above the midpoint (0.5), suggesting that the majority of songs are moderately suitable for dancing.
- The median danceability value is slightly higher than the mean, indicating that there may be some songs in the dataset with relatively high danceability scores that are pulling the median up.
- There are relatively small variance and standard deviation, indicating that the danceability scores in the dataset are not widely spread out from the mean.

ii. Calculate and print mean, median, variance, and standard deviation of Energy column. Then, describe what you find.
"""

mean2 = df['Energy'].mean()
median2 = df['Energy'].median()
variance2 = df['Energy'].var()
std_deviation2 = df['Energy'].std()

print(f'Mean of Energy column is: {mean2: .4f}')
print(f'Median of Energy column is: {median2: .4f}')
print(f'Variance of Energy column is: {variance2: .4f}')
print(f'Standard deviation of Energy column is: {std_deviation2: .4f}')

"""Definition:

Energy: is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. 

Energy column of the dataset says the following:
- The mean energy value means that, on average, the majority of the songs in the dataset have a moderate level of perceived intensity and activity.
- The median is higher than the mean, suggesting that there may be some songs in the dataset with relatively high energy levels that are pulling the median up.
- There are relatively small variance and standard deviation, indicating that the energy values in the dataset are not widely spread out from the mean.

iii. Calculate and print mean, median, variance, and standard deviation of Key column. Then, describe what you find.
"""

mean3 = df['Key'].mean()
median3 = df['Key'].median()
variance3 = df['Key'].var()
std_deviation3 = df['Key'].std()

print(f'Mean of Key column is: {mean3: .4f}')
print(f'Median of Key column is: {median3: .4f}')
print(f'Variance of Key column is: {variance3: .4f}')
print(f'Standard deviation of Key column is: {std_deviation3: .4f}')

"""Definition:

Key: the key the track is in. Integers map to pitches using standard Pitch Class notation. E.g. 0 = C, 1 = C♯/D♭, 2 = D, and so on. If no key was detected, the value is -1.

Key column of the dataset says the following:

- The mean suggests that the average key of the tracks in the dataset is around F♯/G♭ (since 5 represents F and 6 represents G in the pitch class notation).

- The median is lower than the mean value, suggesting that the distribution of the Key column is slightly skewed towards lower values. 

- There are large variance and standard deviation, which indicates that the key signatures of the tracks are quite spread out. In other words, the Key column has a relatively high level of variation.

4. (20 points) Visualize the data.

a. Visualize the data with at least four charts (at least two different types, including bar chart, line chart, scatter chart, etc.). Make sure the visualization is useful for further analysis. 

1st chart: Bar chart (only 1 horizontal bar): Using Views and Stream columns:
Top Ten Songs By Most Views on YouTube and Stream on Spotify.

2nd chart: Bar chart (3 vertical bars): Using Danceability, Energy, and Valence columns:
Danceability, Energy, and Valence of Top Ten Songs By Stream.

3rd chart: 3 Pie charts: Using Album_type, Licensed, and official_video columns: Types of Albums, The Video is Licensed, and The Video is Official.

4th chart: Scatter chart: Using Loudness and Acousticness columns:
Loudness and Acousticness.



b. Your charts should have labels, legends, titles, etc. Use proper colors to make them look nice and professional.

c. Describe what you find given the charts.

i. 1st chart: Bar chart (only 1 horizontal bar): Top Ten Songs By Most Views on YouTube and Stream on Spotify. 

Visualize the data and then analyze it.
"""

grouped = df.groupby(['Views','Stream'], as_index=False)['Track'].sum()

# Sort top ten songs by views and stream.
views = grouped.sort_values('Views', ascending=False)[:10]
stream = grouped.sort_values('Stream', ascending=False)[:10]

print(views)
print(stream)

# Create two subplots.
fig, axs = plt.subplots(2, 1, figsize=(12,6))

x1 = views['Track'].to_numpy()
y1 = views['Views'].to_numpy()

x2 = stream['Track'].to_numpy()
y2 = stream['Stream'].to_numpy()

# Create horizontal bars.
axs[0].barh(x1, y1, color = 'g', height=0.7)
axs[1].barh(x2, y2, color = 'b', height=0.7)

# Set titles for each subplot.
axs[0].set_xlabel('Views (billions) on YouTube', fontdict={'fontsize': 12, 'fontweight': 'bold'})
axs[1].set_xlabel('Stream (billions) on Spotify', fontdict={'fontsize': 12, 'fontweight': 'bold'})

# Invert y-axis on each subplot.
axs[0].invert_yaxis()
axs[1].invert_yaxis()

# Adjust the spacing between the subplots.
plt.subplots_adjust(hspace=0.5)

fig.suptitle('Top Ten Songs By Most Views on YouTube and Stream on Spotify', fontsize=16, fontweight='bold')

plt.show()

"""Definition: 

Track: name of the song.

Views: number of views.

Stream: number of streams of the song on Spotify.

# ***The graph of Top Ten Songs By Most Views on YouTube and Stream on Spotify says the following:***

# ***Need to do this part***

ii. 2nd chart: Bar chart (3 vertical bars): Danceability, Energy, and Valence of Top Ten Songs By Stream.

Visualize the data and then analyze it.
"""

grouped = df.groupby(['Danceability', 'Energy', 'Valence', 'Track'], as_index=False)['Stream'].sum()

# Sort top ten songs by stream.
stream = grouped.sort_values('Stream', ascending=False, ignore_index=True)[:10]

# Create vertical bar chart.
fig, ax = plt.subplots(figsize=(12,6))
ax.bar(stream.index, stream['Danceability'], width=0.3, label='Danceability')
ax.bar(stream.index+0.3, stream['Energy'], width=0.3, label='Energy')
ax.bar(stream.index+0.6, stream['Valence'], width=0.3, label='Valence')

ax.set_title('Danceability, Energy, and Valence of Top Ten Songs By Stream', fontsize=18, fontweight='bold')
ax.set_xlabel('Track', fontsize=14, fontweight='bold')
ax.set_ylabel('Value', fontsize=14, fontweight='bold')

# Set the x-tick labels.
ax.set_xticks(stream.index+0.3)
ax.set_xticklabels(stream['Track'])
plt.xticks(rotation=45, ha='right')

# Add a legend.
ax.legend()

plt.show()

"""Definition: 

Track: name of the song.

Stream: number of streams of the song on Spotify.

Danceability: describes how suitable a track is for dancing based on musical elements. A value of 0.0 is least danceable and 1.0 is most danceable.

Energy: is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. 

Valence: a measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).

# ***The graph of Danceability, Energy, and Valence of Top Ten Songs By Stream says the following:***

# ***Need to do this part***

iii. 3rd chart: 3 Pie charts: Types of Albums, The Video is Licensed, and The Video is Official.

Visualize the data and then analyze it.
"""

# Make a copy of the original data frame.
df_copy = df.copy()

# Filter to keep only 'True' and 'False' values
licensed_filtered = df_copy[df_copy['Licensed'].isin([True, False])]
official_video_filtered = df_copy[df_copy['official_video'].isin([True, False])]

album_type_count = df['Album_type'].value_counts()
licensed_count = licensed_filtered['Licensed'].value_counts()
official_video_count = official_video_filtered['official_video'].value_counts()

labels1 = album_type_count.index.tolist()
sizes1 = album_type_count.values.tolist()
labels2 = licensed_count.index.tolist()
sizes2 = licensed_count.values.tolist()
labels3 = official_video_count.index.tolist()
sizes3 = official_video_count.values.tolist()

# Create two subplots.
fig, axs = plt.subplots(1, 3, figsize=(20,15))

# 'autopct=%1.1f%%' means to display the percentage value rounded to one decimal place.
# 'startangle=90' means that all the slices are rotated counter-clockwise by 90 degrees.
axs[0].pie(sizes1, labels=labels1, autopct='%1.1f%%', startangle=90)
axs[1].pie(sizes2, labels=labels2, autopct='%1.1f%%', startangle=90)
axs[2].pie(sizes3, labels=labels3, autopct='%1.1f%%', startangle=90)

# Set titles for each subplot.
axs[0].set_xlabel('Types of Albums', fontdict={'fontsize': 12, 'fontweight': 'bold'})
axs[1].set_xlabel('The Video is Licensed', fontdict={'fontsize': 12, 'fontweight': 'bold'})
axs[2].set_xlabel('The Video is Official', fontdict={'fontsize': 12, 'fontweight': 'bold'})

# Add a legend and set it to lower right
axs[0].legend(labels1, loc="lower right")
axs[1].legend(labels2, loc="lower right")
axs[2].legend(labels3, loc="lower right")

plt.show()

"""Definition: 

Album_type: indicates if the song is relesead on Spotify as a single or contained in an album.

Licensed: indicates whether the video represents licensed content.

official_video: boolean value that indicates if the video found is the official video of the song.

# ***These graph says the following:***

# ***Need to do this part***

iv. 4th chart: Scatter chart: Loudness and Acousticness.

Visualize the data and then analyze it.
"""

grouped = df.groupby(['Loudness'], as_index=False)['Acousticness'].sum()

# Filter the Acousticness column from 0 to 1.
acousticness = grouped['Acousticness'].tolist()
acousticness_filtered = [x for x in acousticness if 0 <= x <= 1]

# Get the loudness values for the filtered acousticness values.
loudness = grouped.loc[grouped['Acousticness'].isin(acousticness_filtered), 'Loudness'].tolist()

# Create the scatter chart with custom settings.
fig, ax = plt.subplots(figsize=(15, 8))
ax.scatter(loudness, acousticness_filtered, s=20)

plt.title('Loudness and Acousticness', fontsize=18, fontweight='bold')
plt.xlabel('Loudness (db)', fontsize=14, fontweight='bold')
plt.ylabel('Acousticness', fontsize=14, fontweight='bold')

plt.show()

"""Definition: 

Loudness: the overall loudness of a track in decibels (dB). Values typically range between -60 and 0 db.

Acousticness: a confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic.

# ***The graph of Loudness and Acousticness says the following:***

# ***Need to do this part***

5. (10 points, bonus) Build a regression/classification model.

We will build a simple linear regression model using these columns: Likes, Comments, Stream, and Views.

a.	Split the data into training set and validation set.


b.	Show the MSE/accuracy of your model.

i. Visualize the data to find feature variable, which is to find which column is the most correlated to Views column.
"""

import matplotlib.pyplot as plt
import seaborn as sns

grouped = df.groupby(['Likes', 'Comments', 'Stream'], as_index=False)['Views'].sum()

# Visualize the data using scatter plots.
sns.pairplot(grouped, x_vars=['Likes', 'Comments', 'Stream'], y_vars='Views', height=4, aspect=1, kind='scatter')
plt.show()

# Visualize the data using heatmap.
sns.heatmap(grouped.corr(), cmap="YlGnBu", annot = True)
plt.show()

"""As we can see from the above graphs, the Likes column seems most correlated to Views column.

Therefore, let use the Likes column as feature variable.

ii. Create training set and validation set.
"""

# Creating X and y.
X = grouped['Likes']   # Feature variable.
y = grouped['Views']

# Splitting the varaibles as training and validation sets.
from sklearn.model_selection import train_test_split
X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size = 0.7, test_size = 0.3, random_state = 100)

print(f'X_train:')
print(X_train)
print()
print(f'y_train:')
print(y_train)

"""iii. Find and then visualize the regression line."""

print(X_train.shape)
print(X_valid.shape)

# Add additional column to the train and test sets.
X_train = X_train.values.reshape(-1,1)
X_valid = X_valid.values.reshape(-1,1)

from sklearn.linear_model import LinearRegression

# Create an object of Linear Regression.
lm = LinearRegression()

# Fit the model using .fit() method.
lm.fit(X_train, y_train)

# Intercept value.
print("Intercept:",lm.intercept_)

# Slope value.
print('Slope:',lm.coef_)

"""Hence, the regression line is:

Views = 1822709 + 141.4886 * Likes 


"""

# Visualize the regression line.
plt.scatter(X_train, y_train)
plt.plot(X_train, 1822709 + 141.4886*X_train, 'r')

plt.title('Likes and Views Linear Regression', fontsize=18, fontweight='bold')
plt.xlabel('Likes', fontsize=14, fontweight='bold')
plt.ylabel('Views', fontsize=14, fontweight='bold')

plt.show()

"""iv. Find MSE of the model."""

# Make predictions of y_value.
y_train_pred = lm.predict(X_train)
y_valid_pred = lm.predict(X_valid)

from sklearn.metrics import mean_squared_error

# MSE = Mean Squared Error
mse_train = mean_squared_error(y_train,y_train_pred)
mse_valid = mean_squared_error(y_valid,y_valid_pred)

print(f'MSE of training set is: {mse_train: .5f}')
print(f'MSE of valid set is: {mse_valid: .5f}')

# Visualize the line on the validation set with the y predictions.
plt.scatter(X_valid, y_valid)
plt.plot(X_valid, y_valid_pred, 'r')
plt.show()